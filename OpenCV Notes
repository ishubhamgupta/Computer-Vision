***************************************************************************************************************************
Understanding color models and drawing figures on images

The default setting of colour mode in OpenCV comes in the order of BGR, which is different from Matplotlib.
To see the image in RGB mode, we need to convert it from BGR to RGB
A color model is a system for creating a full range of colors using the primary colors.
	a. Additive Color Models
	b. Subtractive Color Models
Additive colors uses light to represent the colors in computer screens viz. Red, Green, Blue
Subtractive colors uses inks to print those digital images on papers viz. Cyan, Magenta, Yellow, Black (CMYK)

A grayscale is simple, it represents images and morphologies by the intensity of black and white, which means it has only one channel
HSV stands for hue, saturation and value. 
HSL stands for hue, saturation and lightness. 
The center axis for HSV is the value of colors while that for HSL is the amount of light. 
Along the angles from the center axis, there is hue, the actual colors. 
And the distance from the center axis belongs to saturation.

Image processing is ‘data preprocessing.’ 
It’s reducing noises and extracting useful patterns to make classification and detection tasks easier. 

**************************************************************************************************************************
The basics of image processing with filtering


When it comes to detecting edges and contours, noise gives a great impact on the accuracy of detection.
Blurring, thresholding, and morphological transformation are the techniques we use for this purpose.
Blurring: The goal of blurring is to perform noise reduction. 

There are several techniques used to achieve blurring effects majorly used ones in OpenCV are:
	1. Averaging Blurring
	2. Gaussian Blurring
	3. Median Blurring
	4. Bilateral Blurring 
All four techniques have a common basic principle, which is applying convolutional operations to the image with a filter (kernel). 
The values of the applying filters are different between the four blurring methods.

Average blurring is taking the average of all the pixel values under the given kernel area and replace the value at the center.
As the size of filters gets bigger, the pixel values will be normalized more. 
Therefore we can expect the image to get blurred the more.

Medium blurring is the same with average blurring except that it uses the median value instead of the average. 
Therefore when we have to handle sudden noises in the image such as ‘salt and pepper noise,’ it’ll be better to use medium blurring than average blurring.
Salt-and-pepper noise is a form of noise sometimes seen on images. It is also known as impulse noise. 
This noise can be caused by sharp and sudden disturbances in the image signal. 
It presents itself as sparsely occurring white and black pixels. 

Gaussian blurring is nothing but using the kernel whose values have a Gaussian distribution. 
The values are generated by a Gaussian function so it requires a sigma value for its parameter.
The values of the kernel go higher near the center and go smaller near the corner. 
It’s good to apply this method to the noises that have a normal distribution such as white noise.


Bilateral Filtering is an advanced version of Gaussian blurring. 
Blurring produces not only dissolving noises but also smoothing edges. 
And bilateral filter can keep edges sharp while removing noises. 
It uses Gaussian-distributed values but takes both distance and the pixel value differences into account. 


Thresholding
Thresholding transforms images into binary images. 
Where we need to set the threshold value and max values and then we convert the pixel values accordingly. 
	1. Binary, 
	2. the inverse of Binary, 
	3. Threshold to zero, 
	4. the inverse of Threshold to Zero, and 
	5. Threshold truncation.

If we have a picture with various amount of lighting in different areas, in this case our approach should be approach would be using different thresholds for each part of the image
Adaptive thresholding, which serves this issue. it calculates the threshold within the neighborhood area of the image, from which we can achieve a better result from images with varying illumination.
We need to convert the color mode to grayscale to apply adaptive thresholding. 

Gradient
The gradient geometrically represents the slope of the graph of a function with multi-variables. 
As it is a vector-valued function, it takes a direction and a magnitude as its components. 
The image gradient represents directional changes in the intensity or color mode and we can use this concept for locating edges.

Sobel operation uses both Gaussian smoothing and differentiation. 
We apply it by cv2.Sobel() and two different directions are available: vertical (sobel_x) and horizontal (sobel_y). dx and dy indicates the derivatives. 
When dx = 1 , the operator calculates the derivatives of the pixel values along the horizontal direction to make a filter.
Laplacian operation uses the second derivatives of x and y

Morphological transformations
It’s also possible to manipulate the figures of images by filtering, which is called as morphological transformation.

Erosion is the technique for shrinking figures and it’s usually processed in a grayscale. 
The shape of filters can be a rectangle, an ellipse, and a cross shape. 
By applying a filter we remove any 0 values under the given area.

Dilation is the opposite to erosion. 
It is making objects expand and the operation will be also opposite to that of erosion.

Opening and closing operation is the mixed version of erosion and dilation. 
Opening performs erosion first and then dilation is performed on the result from the erosion while closing performs dilation first and the erosion.
closing is useful to detect the overall contour of a figure and opening is suitable to detect subpatterns

Gradient filter (MORPH_CGRADIENT) is the subtracted area from dilation to erosion. 
Top hat filter (MORPH_TOPHAT) is the subtracted area from opening to the original image while black hot filter (MORPH_BLACKHAT) is that from closing.

*****************************************************************************************************************************
From feature detection to face detection

Edge detection means identifying points in an image where the brightness changes sharply or discontinuously. 
	1. Detection of edge with low error rate, which means that the detection should accurately catch as many edges shown in the image as possible
	2. The edge point detected from the operator should accurately localize on the center of the edge.
	3. A given edge in the image should only be marked once, and where possible, image noise should not create false edges.
We can draw line segments with those points, which are called edges. 
Edge Detection Techniques - Gradient filtering with Sobel and Laplacian operation. 
By calculating the derivatives of pixel values in a given direction, gradient filtering can depict the edges of images.

Canny detection is another type of edge detection techniques.
Canny edge detection is a technique to extract useful structural information from different vision objects and dramatically reduce the amount of data to be processed. 
Canny has found that the requirements for the application of edge detection on diverse vision systems are relatively similar.  
It’s one of the most popular algorithms for detecting edges, which is performed in four steps: 
	1. Noise reduction, 
	2. Finding gradient and its direction, 
	3. Non-maximum suppression and 
	4. hysteresis thresholding.

The algorithm starts with Gaussian blurring and then it finds the intensity gradient of the image with a Sobel kernel
With the derived gradient and direction, every pixel is checked if a certain point is a local maximum in its surrounding points. 
If it’s not, this point is suppressed to zero (total absence, black). 
This is called non-maximum suppression.
Basically saying when we are detecting something we might end up with many probabilities, the one with max value is being selected and others are suppressed into it

If the point is considered to be a local maximum, it goes to the next stage. 
The final stage is the last decision stage whether the edges at the previous steps are really edges or not. 
This is called Hysteresis Thresholding and we need two threshold values here. 

Given the two different threshold values, we get three ranges of values. 
So if the intensity gradient of a point is higher than the upper threshold, it will be considered as ‘sure-edge.’ 
If the gradient of a point is lower than the lower threshold, the point will be discarded. 
And in case of the gradient being in the middle of the two thresholds, we see its connectivity to other ‘sure-edge’ points. 
If there’s no connection, it will be discarded as well.


Features in images are the points of interest which provide rich image content information. They are basically comprised of two things:
	Interest points : 
	Points in the image which are invariant to rotation, translation, intensity and scale changes. (Basically, robust and reliable). 
	There are different interest points such as corners, edges, blobs etc.
	Feature Descriptors : 
	These describe the image patch around the interest points in vectors. 
	They can be as simple as raw pixel values or complicated like Histogram of Gradients (HoG) etc.
So corner detection is basically detecting (one type of) interest points in an image.

Corner Detection
Corner detection is another detection algorithm which is widely used in object detection, motion detection, video tracking and so on. 
We see a corner as a junction where edges intersect. 
Corners are locations in images where a slight shift in the location will lead to a large change in intensity in both horizontal (X) and vertical (Y) axes.

Harris Corner Detector
STEP 1. It determines which windows (small image patches) produce very large variations in intensity when moved in both X and Y directions (i.e. gradients).
STEP 2. With each such window found, a score R is computed.
STEP 3. After applying a threshold to this score, important corners are selected & marked.

Note: Harris Detector is not scale-invariant

Shi-Tomasi Corner Detector
Shi-Tomasi is almost similar to Harris Corner detector, apart from the way the score (R) is calculated. 
This gives a better result. 
Moreover, in this method, we can find the top N corners, which might be useful in case we don’t want to detect each and every corner. 
If R is greater than a threshold, its classified as a corner.

Conclusion:
Shi-Tomasi is a slightly better version after just changing the score formula.
We detect corners for several applications : image alignment, image stitching (remember the panorama feature on your phone camera?), object recognition, 3D reconstruction, motion tracking and so on


Face Detection:
Face detection is a technology identifying the presence and the position of human faces in digital images.
I want you to differentiate face detection from face recognition which indicates detecting the identification of a person by his or her face. 
So face detection can’t tell us to whom the detected face belongs.

Face detection is basically a classification task so it’s trained to classify whether there is a target object or not. 
And Haar Feature-based Cascade Classifier is one of the face detection models available in OpenCV. 
This is a pre-trained model, which means it already completed training with thousands of images. 
The 4 key points for understanding this algorithm are: 
	1. Haar features extraction, 
	2. integral image, 
	3. Adaboost and 
	4. cascade classifiers.

While detecting there are lots of features to be noticed:
	1. Edge Feature
	2. Line Feature
	3. Centre surround Features
	4. Four-rectangle Features

Haar-like features are image filters or image kernels used in object detection and the examples are shown above. 
They owe their name to their intuitive similarity with Haar wavelets which is originally proposed by Alfréd Haar. 
During detection, we pass the window on an image and do the convolutional operation with the filters to see if there’s the feature we’re looking for is in the image.
Basically, mean of dark region minus mean of light region

The integral image is a way of image representation which is derived to make the feature evaluation faster and more effective.

Think about when the detecting window is at the blank background where there’s no object or face. 
It’ll still be a waste of time if it performs the same process in such a part.


A cascade classifier constructs stepwise stages and gives an order among Haar-like features. 
Basic forms of the features are implemented at the early stages and the more complex ones are applied only for those promising regions. And at each stage, the Adaboost model will be trained by ensembling weak learners. If a subpart, or a sub-window, is classified as ‘not a face-like region’ at the prior stage, it’s rejected to the next step. 
By doing so, we can only consider the survived ones and achieve much higher speed.

*************************************************************************************************************
Contour Detection

Contour detection which is essential in object detection
A contour line indicates a curved line representing the boundary of the same values or the same intensities.

What’s the difference between edges and contours? 
The two terms are often used interchangeably so it could be a bit confusing. 
To put it simply, the concept of edges lies in a local range while the concept of contours is at the overall boundary of a figure. 
Edges are points whose values change significantly compared to their neighboring points. 
Contours, on the other hand, are closed curves which are obtained from edges and depicting a boundary of figures.


We can find the centroid of an image or calculate the area of a boundary field with the help of the notion called image moment.
The word ‘moment’ is a short period of time in common usage.
In physics terminology, a moment is the product of the distance and another physical quantity meaning how a physical quantity is distributed or located. 
So in computer vision, Image moment is how image pixel intensities are distributed according to their location. 
It’s a weighted average of image pixel intensities and we can get the centroid or spatial information from the image moment.

There are three types of moments- 
	1. spatial moments, 
	2. central moments, and 
	3. central normalized moments.

